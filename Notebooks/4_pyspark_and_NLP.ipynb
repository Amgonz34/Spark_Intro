{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_pyspark_and_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KAp7FtDVIXAl"
      },
      "outputs": [],
      "source": [
        "# #Install Spark and Java\n",
        "# ! pip install pyspark\n",
        "# !apt-get update\n",
        "# !apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "# !tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark\n",
        "# !pip install nltk\n",
        "# !python -m nltk.downloader popular"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = 'spark-3.2.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"Spark_NLP\").getOrCreate()"
      ],
      "metadata": {
        "id": "KlIcvxuFIlkA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using NLP to Tokenization and Part-of-Speech Tagging\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "text = word_tokenize(\"I enjoy biking on the trails\")\n",
        "output = nltk.pos_tag(text)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2MnLuB_Iltg",
        "outputId": "01c0661c-918c-426b-835d-f9dd3c62eca3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('enjoy', 'VBP'), ('biking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('trails', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer"
      ],
      "metadata": {
        "id": "U-4AG7w8Kiw0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sample df\n",
        "dataframe = spark.createDataFrame([\n",
        "    (0, \"spark is great\"),\n",
        "    (1, \"I like learning spark\"),\n",
        "    (2, \"Why learn hadoop when you can learn spark\")\n",
        "], [\"id\",\"sentence\"])\n",
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF5Pbt_XKi3u",
        "outputId": "3b9e37f0-80ad-46ea-90bc-8953a35283d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|      spark is great|\n",
            "|  1|I like learning s...|\n",
            "|  2|Why learn hadoop ...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sentences\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRaM92gVKi8e",
        "outputId": "172eba0b-dfe4-47e8-f90e-99556e0fa0da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer_221cb1b077fe"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_df = tokenizer.transform(dataframe)\n",
        "tokenized_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0nU1JsqMNQU",
        "outputId": "af5ad242-fb61-40e0-d4f9-69f93b09e90f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------------------+--------------------------------------------------+\n",
            "|id |sentence                                 |words                                             |\n",
            "+---+-----------------------------------------+--------------------------------------------------+\n",
            "|0  |spark is great                           |[spark, is, great]                                |\n",
            "|1  |I like learning spark                    |[i, like, learning, spark]                        |\n",
            "|2  |Why learn hadoop when you can learn spark|[why, learn, hadoop, when, you, can, learn, spark]|\n",
            "+---+-----------------------------------------+--------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "metadata": {
        "id": "SjKJeX6sMNVU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_list_length(word_list):\n",
        "    return len(word_list)\n",
        "def word_unique_list_length(word_list):\n",
        "    return len(set(word_list))\n",
        "# Create a user defined function\n",
        "count_tokens = udf(word_list_length, IntegerType())\n",
        "count_unique_tokens = udf(word_unique_list_length, IntegerType())"
      ],
      "metadata": {
        "id": "xRbp-DW4M0ke"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_df = tokenized_df.withColumn(\"tokens\",count_tokens(col(\"words\")))\n",
        "tokenized_df = tokenized_df.withColumn(\"unique_tokens\",count_unique_tokens(col(\"words\")))\n",
        "tokenized_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZza301hM0ni",
        "outputId": "e9462dd4-0a40-417e-de12-57fcc5992b7d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+------+-------------+\n",
            "| id|            sentence|               words|tokens|unique_tokens|\n",
            "+---+--------------------+--------------------+------+-------------+\n",
            "|  0|      spark is great|  [spark, is, great]|     3|            3|\n",
            "|  1|I like learning s...|[i, like, learnin...|     4|            4|\n",
            "|  2|Why learn hadoop ...|[why, learn, hado...|     8|            7|\n",
            "+---+--------------------+--------------------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make df with stops words\n",
        "dataframe_stopwords = spark.createDataFrame([\n",
        "    (0, \"I use spark with big data\"),\n",
        "    (1, \"This stop words will be removed\"),\n",
        "    (2, \"This is going to be cool\")\n",
        "], [\"id\",\"sentence\"])\n",
        "dataframe_stopwords.show()"
      ],
      "metadata": {
        "id": "YyX_xF1ZM0sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd42a71-26b4-4bb0-a1d1-21fad53a48dd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|            sentence|\n",
            "+---+--------------------+\n",
            "|  0|I use spark with ...|\n",
            "|  1|This stop words w...|\n",
            "|  2|This is going to ...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stop words library\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "# Run the Remover\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")"
      ],
      "metadata": {
        "id": "YFLyyQ6pM0vl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataframe_stopwords = tokenizer.transform(dataframe_stopwords)\n",
        "tokenized_dataframe_stopwords.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGTmuexi0DPz",
        "outputId": "250861a9-8b1f-411c-9792-0cf7e3b40ebc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------+--------------------------------------+\n",
            "|id |sentence                       |words                                 |\n",
            "+---+-------------------------------+--------------------------------------+\n",
            "|0  |I use spark with big data      |[i, use, spark, with, big, data]      |\n",
            "|1  |This stop words will be removed|[this, stop, words, will, be, removed]|\n",
            "|2  |This is going to be cool       |[this, is, going, to, be, cool]       |\n",
            "+---+-------------------------------+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remover.transform(tokenized_dataframe_stopwords).show(truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt0wChjQzhqr",
        "outputId": "4ba3256c-1cd8-46f3-d72e-8c6c8b24ecca"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+--------------------+\n",
            "| id|            sentence|               words|            filtered|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "|  0|I use spark with ...|[i, use, spark, w...|[use, spark, big,...|\n",
            "|  1|This stop words w...|[this, stop, word...|[stop, words, rem...|\n",
            "|  2|This is going to ...|[this, is, going,...|       [going, cool]|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}